{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/avd1729/notebookbf5fa64b11?scriptVersionId=143899491\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# **Setting up the environment**","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2023-09-22T17:08:05.434518Z","iopub.execute_input":"2023-09-22T17:08:05.43507Z","iopub.status.idle":"2023-09-22T17:08:05.459506Z","shell.execute_reply.started":"2023-09-22T17:08:05.435016Z","shell.execute_reply":"2023-09-22T17:08:05.458187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2023-09-22T17:08:05.463796Z","iopub.execute_input":"2023-09-22T17:08:05.4653Z","iopub.status.idle":"2023-09-22T17:08:14.792889Z","shell.execute_reply.started":"2023-09-22T17:08:05.465255Z","shell.execute_reply":"2023-09-22T17:08:14.791714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Loading the data**","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\n\nurl = \"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\npath = tf.keras.utils.get_file(\"spa-eng.zip\", origin=url, cache_dir=\"datasets\",\n                               extract=True)\ntext = (Path(path).with_name(\"spa-eng\") / \"spa.txt\").read_text()","metadata":{"execution":{"iopub.status.busy":"2023-09-22T17:08:14.794793Z","iopub.execute_input":"2023-09-22T17:08:14.795614Z","iopub.status.idle":"2023-09-22T17:08:15.107311Z","shell.execute_reply.started":"2023-09-22T17:08:14.795573Z","shell.execute_reply":"2023-09-22T17:08:15.106202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random.seed(42)\n\ntext = text.replace(\"¡\", \"\").replace(\"¿\", \"\")\npairs = [line.split(\"\\t\") for line in text.splitlines()]\nnp.random.shuffle(pairs)\nsentences_en, sentences_es = zip(*pairs)  # separates the pairs into 2 lists","metadata":{"execution":{"iopub.status.busy":"2023-09-22T17:08:15.110412Z","iopub.execute_input":"2023-09-22T17:08:15.110955Z","iopub.status.idle":"2023-09-22T17:08:15.76531Z","shell.execute_reply.started":"2023-09-22T17:08:15.110904Z","shell.execute_reply":"2023-09-22T17:08:15.764128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(3):\n    print(sentences_en[i], \"=>\", sentences_es[i])","metadata":{"execution":{"iopub.status.busy":"2023-09-22T17:08:15.767172Z","iopub.execute_input":"2023-09-22T17:08:15.767599Z","iopub.status.idle":"2023-09-22T17:08:15.778583Z","shell.execute_reply.started":"2023-09-22T17:08:15.76756Z","shell.execute_reply":"2023-09-22T17:08:15.777312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size = 1000\nmax_length = 50\ntext_vec_layer_en = tf.keras.layers.TextVectorization(\n    vocab_size, output_sequence_length=max_length)\ntext_vec_layer_es = tf.keras.layers.TextVectorization(\n    vocab_size, output_sequence_length=max_length)\ntext_vec_layer_en.adapt(sentences_en)\ntext_vec_layer_es.adapt([f\"startofseq {s} endofseq\" for s in sentences_es])","metadata":{"execution":{"iopub.status.busy":"2023-09-22T17:08:15.780686Z","iopub.execute_input":"2023-09-22T17:08:15.781162Z","iopub.status.idle":"2023-09-22T17:08:40.165126Z","shell.execute_reply.started":"2023-09-22T17:08:15.78112Z","shell.execute_reply":"2023-09-22T17:08:40.163864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_vec_layer_en.get_vocabulary()[:10]","metadata":{"execution":{"iopub.status.busy":"2023-09-22T17:08:40.167773Z","iopub.execute_input":"2023-09-22T17:08:40.168647Z","iopub.status.idle":"2023-09-22T17:08:40.185956Z","shell.execute_reply.started":"2023-09-22T17:08:40.168606Z","shell.execute_reply":"2023-09-22T17:08:40.184819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_vec_layer_es.get_vocabulary()[:10]","metadata":{"execution":{"iopub.status.busy":"2023-09-22T17:08:40.187527Z","iopub.execute_input":"2023-09-22T17:08:40.188477Z","iopub.status.idle":"2023-09-22T17:08:40.202527Z","shell.execute_reply.started":"2023-09-22T17:08:40.188427Z","shell.execute_reply":"2023-09-22T17:08:40.201358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = tf.constant(sentences_en[:100_000])\nX_valid = tf.constant(sentences_en[100_000:])\nX_train_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[:100_000]])\nX_valid_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[100_000:]])\nY_train = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[:100_000]])\nY_valid = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[100_000:]])","metadata":{"execution":{"iopub.status.busy":"2023-09-22T17:08:40.204086Z","iopub.execute_input":"2023-09-22T17:08:40.204581Z","iopub.status.idle":"2023-09-22T17:08:41.410489Z","shell.execute_reply.started":"2023-09-22T17:08:40.204541Z","shell.execute_reply":"2023-09-22T17:08:41.409575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.random.set_seed(42) \nencoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\ndecoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)","metadata":{"execution":{"iopub.status.busy":"2023-09-22T17:08:41.414586Z","iopub.execute_input":"2023-09-22T17:08:41.417213Z","iopub.status.idle":"2023-09-22T17:08:41.437401Z","shell.execute_reply.started":"2023-09-22T17:08:41.417175Z","shell.execute_reply":"2023-09-22T17:08:41.436294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embed_size = 128\nencoder_input_ids = text_vec_layer_en(encoder_inputs)\ndecoder_input_ids = text_vec_layer_es(decoder_inputs)\nencoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n                                                    mask_zero=True)\ndecoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n                                                    mask_zero=True)\nencoder_embeddings = encoder_embedding_layer(encoder_input_ids)\ndecoder_embeddings = decoder_embedding_layer(decoder_input_ids)","metadata":{"execution":{"iopub.status.busy":"2023-09-22T17:08:41.440328Z","iopub.execute_input":"2023-09-22T17:08:41.441038Z","iopub.status.idle":"2023-09-22T17:08:41.555054Z","shell.execute_reply.started":"2023-09-22T17:08:41.440998Z","shell.execute_reply":"2023-09-22T17:08:41.553927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Encoder-Decoder Network**","metadata":{}},{"cell_type":"code","source":"encoder = tf.keras.layers.LSTM(512, return_state=True)\nencoder_outputs, *encoder_state = encoder(encoder_embeddings)","metadata":{"execution":{"iopub.status.busy":"2023-09-22T17:08:41.556664Z","iopub.execute_input":"2023-09-22T17:08:41.55712Z","iopub.status.idle":"2023-09-22T17:08:42.773721Z","shell.execute_reply.started":"2023-09-22T17:08:41.55708Z","shell.execute_reply":"2023-09-22T17:08:42.772636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"decoder = tf.keras.layers.LSTM(512, return_sequences=True)\ndecoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)","metadata":{"execution":{"iopub.status.busy":"2023-09-22T17:08:42.775663Z","iopub.execute_input":"2023-09-22T17:08:42.776116Z","iopub.status.idle":"2023-09-22T17:08:43.905913Z","shell.execute_reply.started":"2023-09-22T17:08:42.776078Z","shell.execute_reply":"2023-09-22T17:08:43.904733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\nY_proba = output_layer(decoder_outputs)","metadata":{"execution":{"iopub.status.busy":"2023-09-22T17:08:43.908048Z","iopub.execute_input":"2023-09-22T17:08:43.90846Z","iopub.status.idle":"2023-09-22T17:08:43.948969Z","shell.execute_reply.started":"2023-09-22T17:08:43.908421Z","shell.execute_reply":"2023-09-22T17:08:43.947814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n                       outputs=[Y_proba])\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n              metrics=[\"accuracy\"])\nmodel.fit((X_train, X_train_dec), Y_train, epochs=10,\n          validation_data=((X_valid, X_valid_dec), Y_valid))","metadata":{"execution":{"iopub.status.busy":"2023-09-22T17:08:43.950344Z","iopub.execute_input":"2023-09-22T17:08:43.95077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def translate(sentence_en):\n    translation = \"\"\n    for word_idx in range(max_length):\n        X = np.array([sentence_en])  # encoder input \n        X_dec = np.array([\"startofseq \" + translation])  # decoder input\n        y_proba = model.predict((X, X_dec))[0, word_idx]  # last token's probas\n        predicted_word_id = np.argmax(y_proba)\n        predicted_word = text_vec_layer_es.get_vocabulary()[predicted_word_id]\n        if predicted_word == \"endofseq\":\n            break\n        translation += \" \" + predicted_word\n    return translation.strip()","metadata":{"execution":{"iopub.status.busy":"2023-09-22T17:09:45.317656Z","iopub.execute_input":"2023-09-22T17:09:45.318138Z","iopub.status.idle":"2023-09-22T17:09:45.333555Z","shell.execute_reply.started":"2023-09-22T17:09:45.318102Z","shell.execute_reply":"2023-09-22T17:09:45.332178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"translate(\"I like Messi\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"translate(\"I like football and also going to the beach\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Bidirectional RNNs**","metadata":{}},{"cell_type":"code","source":"tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\nencoder = tf.keras.layers.Bidirectional(\n    tf.keras.layers.LSTM(256, return_state=True))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder_outputs, *encoder_state = encoder(encoder_embeddings)\nencoder_state = [tf.concat(encoder_state[::2], axis=-1),  # short-term (0 & 2)\n                 tf.concat(encoder_state[1::2], axis=-1)]  # long-term (1 & 3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"decoder = tf.keras.layers.LSTM(512, return_sequences=True)\ndecoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)\noutput_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\nY_proba = output_layer(decoder_outputs)\nmodel = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n                       outputs=[Y_proba])\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n              metrics=[\"accuracy\"])\nmodel.fit((X_train, X_train_dec), Y_train, epochs=10,\n          validation_data=((X_valid, X_valid_dec), Y_valid))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"translate(\"I like Messi\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"translate(\"I like football and also going to the beach\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Beam Search**","metadata":{}},{"cell_type":"code","source":"def beam_search(sentence_en, beam_width, verbose=False):\n    X = np.array([sentence_en])  # encoder input\n    X_dec = np.array([\"startofseq\"])  # decoder input\n    y_proba = model.predict((X, X_dec))[0, 0]  # first token's probas\n    top_k = tf.math.top_k(y_proba, k=beam_width)\n    top_translations = [  # list of best (log_proba, translation)\n        (np.log(word_proba), text_vec_layer_es.get_vocabulary()[word_id])\n        for word_proba, word_id in zip(top_k.values, top_k.indices)\n    ]\n    \n    # extra code – displays the top first words in verbose mode\n    if verbose:\n        print(\"Top first words:\", top_translations)\n\n    for idx in range(1, max_length):\n        candidates = []\n        for log_proba, translation in top_translations:\n            if translation.endswith(\"endofseq\"):\n                candidates.append((log_proba, translation))\n                continue  # translation is finished, so don't try to extend it\n            X = np.array([sentence_en])  # encoder input\n            X_dec = np.array([\"startofseq \" + translation])  # decoder input\n            y_proba = model.predict((X, X_dec))[0, idx]  # last token's proba\n            for word_id, word_proba in enumerate(y_proba):\n                word = text_vec_layer_es.get_vocabulary()[word_id]\n                candidates.append((log_proba + np.log(word_proba),\n                                   f\"{translation} {word}\"))\n        top_translations = sorted(candidates, reverse=True)[:beam_width]\n\n        # extra code – displays the top translation so far in verbose mode\n        if verbose:\n            print(\"Top translations so far:\", top_translations)\n\n        if all([tr.endswith(\"endofseq\") for _, tr in top_translations]):\n            return top_translations[0][1].replace(\"endofseq\", \"\").strip()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence_en = \"I love cats and dogs\"\ntranslate(sentence_en)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"beam_search(sentence_en, beam_width=3, verbose=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Attention Mechanisms**","metadata":{}},{"cell_type":"code","source":"tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\nencoder = tf.keras.layers.Bidirectional(\n    tf.keras.layers.LSTM(256, return_sequences=True, return_state=True))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder_outputs, *encoder_state = encoder(encoder_embeddings)\nencoder_state = [tf.concat(encoder_state[::2], axis=-1),  # short-term (0 & 2)\n                 tf.concat(encoder_state[1::2], axis=-1)]  # long-term (1 & 3)\ndecoder = tf.keras.layers.LSTM(512, return_sequences=True)\ndecoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"attention_layer = tf.keras.layers.Attention()\nattention_outputs = attention_layer([decoder_outputs, encoder_outputs])\noutput_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\nY_proba = output_layer(attention_outputs)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n                       outputs=[Y_proba])\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n              metrics=[\"accuracy\"])\nmodel.fit((X_train, X_train_dec), Y_train, epochs=10,\n          validation_data=((X_valid, X_valid_dec), Y_valid))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"translate(\"I like football and also going to the beach\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"beam_search(\"I like football and also going to the beach\", beam_width=3,\n            verbose=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Transformer Architecture**","metadata":{}},{"cell_type":"code","source":"max_length = 50  # max length in the whole training set\nembed_size = 128\ntf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\npos_embed_layer = tf.keras.layers.Embedding(max_length, embed_size)\nbatch_max_len_enc = tf.shape(encoder_embeddings)[1]\nencoder_in = encoder_embeddings + pos_embed_layer(tf.range(batch_max_len_enc))\nbatch_max_len_dec = tf.shape(decoder_embeddings)[1]\ndecoder_in = decoder_embeddings + pos_embed_layer(tf.range(batch_max_len_dec))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PositionalEncoding(tf.keras.layers.Layer):\n    def __init__(self, max_length, embed_size, dtype=tf.float32, **kwargs):\n        super().__init__(dtype=dtype, **kwargs)\n        assert embed_size % 2 == 0, \"embed_size must be even\"\n        p, i = np.meshgrid(np.arange(max_length),\n                           2 * np.arange(embed_size // 2))\n        pos_emb = np.empty((1, max_length, embed_size))\n        pos_emb[0, :, ::2] = np.sin(p / 10_000 ** (i / embed_size)).T\n        pos_emb[0, :, 1::2] = np.cos(p / 10_000 ** (i / embed_size)).T\n        self.pos_encodings = tf.constant(pos_emb.astype(self.dtype))\n        self.supports_masking = True\n\n    def call(self, inputs):\n        batch_max_length = tf.shape(inputs)[1]\n        return inputs + self.pos_encodings[:, :batch_max_length]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pos_embed_layer = PositionalEncoding(max_length, embed_size)\nencoder_in = pos_embed_layer(encoder_embeddings)\ndecoder_in = pos_embed_layer(decoder_embeddings)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfigure_max_length = 201\nfigure_embed_size = 512\npos_emb = PositionalEncoding(figure_max_length, figure_embed_size)\nzeros = np.zeros((1, figure_max_length, figure_embed_size), np.float32)\nP = pos_emb(zeros)[0].numpy()\ni1, i2, crop_i = 100, 101, 150\np1, p2, p3 = 22, 60, 35\nfig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(9, 5))\nax1.plot([p1, p1], [-1, 1], \"k--\", label=\"$p = {}$\".format(p1))\nax1.plot([p2, p2], [-1, 1], \"k--\", label=\"$p = {}$\".format(p2), alpha=0.5)\nax1.plot(p3, P[p3, i1], \"bx\", label=\"$p = {}$\".format(p3))\nax1.plot(P[:,i1], \"b-\", label=\"$i = {}$\".format(i1))\nax1.plot(P[:,i2], \"r-\", label=\"$i = {}$\".format(i2))\nax1.plot([p1, p2], [P[p1, i1], P[p2, i1]], \"bo\")\nax1.plot([p1, p2], [P[p1, i2], P[p2, i2]], \"ro\")\nax1.legend(loc=\"center right\", fontsize=14, framealpha=0.95)\nax1.set_ylabel(\"$P_{(p,i)}$\", rotation=0, fontsize=16)\nax1.grid(True, alpha=0.3)\nax1.hlines(0, 0, figure_max_length - 1, color=\"k\", linewidth=1, alpha=0.3)\nax1.axis([0, figure_max_length - 1, -1, 1])\nax2.imshow(P.T[:crop_i], cmap=\"gray\", interpolation=\"bilinear\", aspect=\"auto\")\nax2.hlines(i1, 0, figure_max_length - 1, color=\"b\", linewidth=3)\ncheat = 2  # need to raise the red line a bit, or else it hides the blue one\nax2.hlines(i2+cheat, 0, figure_max_length - 1, color=\"r\", linewidth=3)\nax2.plot([p1, p1], [0, crop_i], \"k--\")\nax2.plot([p2, p2], [0, crop_i], \"k--\", alpha=0.5)\nax2.plot([p1, p2], [i2+cheat, i2+cheat], \"ro\")\nax2.plot([p1, p2], [i1, i1], \"bo\")\nax2.axis([0, figure_max_length - 1, 0, crop_i])\nax2.set_xlabel(\"$p$\", fontsize=16)\nax2.set_ylabel(\"$i$\", rotation=0, fontsize=16)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Multi-Head Attention**","metadata":{}},{"cell_type":"code","source":"N = 2  # instead of 6\nnum_heads = 8\ndropout_rate = 0.1\nn_units = 128  # for the first Dense layer in each Feed Forward block\nencoder_pad_mask = tf.math.not_equal(encoder_input_ids, 0)[:, tf.newaxis]\nZ = encoder_in\nfor _ in range(N):\n    skip = Z\n    attn_layer = tf.keras.layers.MultiHeadAttention(\n        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n    Z = attn_layer(Z, value=Z, attention_mask=encoder_pad_mask)\n    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n    skip = Z\n    Z = tf.keras.layers.Dense(n_units, activation=\"relu\")(Z)\n    Z = tf.keras.layers.Dense(embed_size)(Z)\n    Z = tf.keras.layers.Dropout(dropout_rate)(Z)\n    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"decoder_pad_mask = tf.math.not_equal(decoder_input_ids, 0)[:, tf.newaxis]\ncausal_mask = tf.linalg.band_part(  # creates a lower triangular matrix\n    tf.ones((batch_max_len_dec, batch_max_len_dec), tf.bool), -1, 0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder_outputs = Z  # let's save the encoder's final outputs\nZ = decoder_in  # the decoder starts with its own inputs\nfor _ in range(N):\n    skip = Z\n    attn_layer = tf.keras.layers.MultiHeadAttention(\n        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n    Z = attn_layer(Z, value=Z, attention_mask=causal_mask & decoder_pad_mask)\n    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n    skip = Z\n    attn_layer = tf.keras.layers.MultiHeadAttention(\n        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n    Z = attn_layer(Z, value=encoder_outputs, attention_mask=encoder_pad_mask)\n    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n    skip = Z\n    Z = tf.keras.layers.Dense(n_units, activation=\"relu\")(Z)\n    Z = tf.keras.layers.Dense(embed_size)(Z)\n    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_proba = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")(Z)\nmodel = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n                       outputs=[Y_proba])\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n              metrics=[\"accuracy\"])\nmodel.fit((X_train, X_train_dec), Y_train, epochs=10,\n          validation_data=((X_valid, X_valid_dec), Y_valid))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"translate(\"I like football and also going to the beach\")","metadata":{},"execution_count":null,"outputs":[]}]}